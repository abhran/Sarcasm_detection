{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Bert_Highest+SVC.ipynb","provenance":[{"file_id":"1FdZjlf-71sxhRiAzpixL6KhFr69JBLyW","timestamp":1621951456958},{"file_id":"1ftGKQ22ZBSIX_d6KEpUI7vRczYgtQPMF","timestamp":1620983839237},{"file_id":"https://github.com/prateekjoshi565/Fine-Tuning-BERT/blob/master/Fine_Tuning_BERT_for_Spam_Classification.ipynb","timestamp":1619947106006}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"OFOTiqrtNvyy"},"source":["# Install Transformers Library"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1hkhc10wNrGt","executionInfo":{"elapsed":5112,"status":"ok","timestamp":1621953320869,"user":{"displayName":"ABHISHEK RANJAN","photoUrl":"","userId":"15498253178039914064"},"user_tz":420},"outputId":"596e7eb6-3532-4458-a474-f03062ac7d8d"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (8.0.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"x4giRzM7NtHJ"},"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import transformers\n","from transformers import AutoModel, BertTokenizerFast\n","\n","# specify GPU\n","device = torch.device(\"cuda\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7sYuExU3YiR4","executionInfo":{"elapsed":30,"status":"ok","timestamp":1621953320870,"user":{"displayName":"ABHISHEK RANJAN","photoUrl":"","userId":"15498253178039914064"},"user_tz":420},"outputId":"32d52290-fe03-4360-f10f-4106e2e247d9"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kKd-Tj3hOMsZ"},"source":["# Load Dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":197},"id":"cwJrQFQgN_BE","executionInfo":{"elapsed":28,"status":"ok","timestamp":1621953320871,"user":{"displayName":"ABHISHEK RANJAN","photoUrl":"","userId":"15498253178039914064"},"user_tz":420},"outputId":"cc624067-36d5-454d-9323-b76ecd2a2ac2"},"source":["df = pd.read_json('/content/drive/MyDrive/sercasm/twitter/sarcasm_detection_shared_task_twitter_training.jsonl',lines=True)\n","df1 = pd.read_json('/content/drive/MyDrive/sercasm/twitter/sarcasm_detection_shared_task_twitter_testing.jsonl',lines=True)\n","\n","df['labels'] = df['label'].apply(lambda x: ['SARCASM', 'NOT_SARCASM'].index(x))\n","df1['labels'] = df1['label'].apply(lambda x: ['SARCASM', 'NOT_SARCASM'].index(x))\n","df1.tail()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>context</th>\n","      <th>response</th>\n","      <th>id</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1795</th>\n","      <td>NOT_SARCASM</td>\n","      <td>[I have been a business customer of MWeb @USER...</td>\n","      <td>@USER @USER @USER is definitely the best out t...</td>\n","      <td>twitter_1796</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1796</th>\n","      <td>SARCASM</td>\n","      <td>[A woman refuses to have her temperature taken...</td>\n","      <td>@USER @USER Ye let her out run wild and infect...</td>\n","      <td>twitter_1797</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1797</th>\n","      <td>SARCASM</td>\n","      <td>[The reason big government wants @USER out is ...</td>\n","      <td>@USER @USER @USER Thanks for that , I would ha...</td>\n","      <td>twitter_1798</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1798</th>\n","      <td>NOT_SARCASM</td>\n","      <td>[Happy #musicmonday and #thanks for #all your ...</td>\n","      <td>@USER @USER @USER Yes also #found this on #new...</td>\n","      <td>twitter_1799</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1799</th>\n","      <td>NOT_SARCASM</td>\n","      <td>[Not long wrapped on the amazing #January22nd ...</td>\n","      <td>@USER @USER @USER you still need to send the l...</td>\n","      <td>twitter_1800</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            label  ... labels\n","1795  NOT_SARCASM  ...      1\n","1796      SARCASM  ...      0\n","1797      SARCASM  ...      0\n","1798  NOT_SARCASM  ...      1\n","1799  NOT_SARCASM  ...      1\n","\n","[5 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":60}]},{"cell_type":"code","metadata":{"id":"U7WBye79ZKoO"},"source":["df[\"tweets\"]=df['response']\n","df1[\"tweets\"]=df1['response']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fzPPOrVQWiW5","executionInfo":{"elapsed":20,"status":"ok","timestamp":1621953320872,"user":{"displayName":"ABHISHEK RANJAN","photoUrl":"","userId":"15498253178039914064"},"user_tz":420},"outputId":"f8412a48-e42f-43e7-991b-c56d90f55fdb"},"source":["df.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5000, 5)"]},"metadata":{"tags":[]},"execution_count":62}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"676DPU1BOPdp","executionInfo":{"elapsed":17,"status":"ok","timestamp":1621953320873,"user":{"displayName":"ABHISHEK RANJAN","photoUrl":"","userId":"15498253178039914064"},"user_tz":420},"outputId":"f86d790a-6f69-4b48-dbf5-11a1ae5935c5"},"source":["# check class distribution\n","df['labels'].value_counts(normalize = True)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1    0.5\n","0    0.5\n","Name: labels, dtype: float64"]},"metadata":{"tags":[]},"execution_count":63}]},{"cell_type":"markdown","metadata":{"id":"MKfWnApvOoE7"},"source":["# Split train dataset into train, validation and test sets"]},{"cell_type":"code","metadata":{"id":"mfhSPF5jOWb7"},"source":["\n","X_s = df['tweets'].values\n","y_s = df['labels'].values\n","\n","Xt_s = df1['tweets'].values\n","yt_s = df1['labels'].values\n","\n","train_text, temp_text, train_labels, temp_labels = X_s,Xt_s,y_s,yt_s\n","\n","# we will use temp_text and temp_labels to create validation and test set\n","val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n","                                                                random_state=2018, \n","                                                                test_size=0.5, \n","                                                                stratify=temp_labels)\n","val_text, test_text, val_labels, test_labels=temp_text, temp_text, temp_labels, temp_labels\n","\n","# X_train, X_test, y_train, y_test = train_test_split(X_s, y_s, test_size=0.2, random_state=42)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W1jEFb39cCjk"},"source":["# pip install -U sentence-transformers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n7hsdLoCO7uB"},"source":["# Import BERT Model and BERT Tokenizer"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S1kY3gZjO2RE","executionInfo":{"elapsed":1037,"status":"ok","timestamp":1621953321897,"user":{"displayName":"ABHISHEK RANJAN","photoUrl":"","userId":"15498253178039914064"},"user_tz":420},"outputId":"59dcc331-b315-4a3a-e602-adbe90f032ab"},"source":["# import BERT-base pretrained model\n","bert = AutoModel.from_pretrained('bert-base-uncased')\n","\n","# Load the BERT tokenizer\n","tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"_zOKeOMeO-DT"},"source":["# sample data\n","text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n","# encode text\n","sent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oAH73n39PHLw","executionInfo":{"elapsed":33,"status":"ok","timestamp":1621953321906,"user":{"displayName":"ABHISHEK RANJAN","photoUrl":"","userId":"15498253178039914064"},"user_tz":420},"outputId":"d5c7b3da-109d-4428-ae1b-63350adfd1e4"},"source":["# output\n","print(sent_id)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'input_ids': [[101, 2023, 2003, 1037, 14324, 2944, 14924, 4818, 102, 0], [101, 2057, 2097, 2986, 1011, 8694, 1037, 14324, 2944, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8wIYaWI_Prg8"},"source":["# Tokenization"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":281},"id":"yKwbpeN_PMiu","executionInfo":{"elapsed":28,"status":"ok","timestamp":1621953321907,"user":{"displayName":"ABHISHEK RANJAN","photoUrl":"","userId":"15498253178039914064"},"user_tz":420},"outputId":"f4b47989-4779-4fdb-b301-8a6a28f63d82"},"source":["# get length of all the messages in the train set\n","seq_len = [len(i.split()) for i in train_text]\n","\n","pd.Series(seq_len).hist(bins = 30)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7ff0b77ae450>"]},"metadata":{"tags":[]},"execution_count":69},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"OXcswEIRPvGe"},"source":["max_seq_len = 100"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tk5S7DWaP2t6","executionInfo":{"elapsed":733,"status":"ok","timestamp":1621953322622,"user":{"displayName":"ABHISHEK RANJAN","photoUrl":"","userId":"15498253178039914064"},"user_tz":420},"outputId":"5cba40d9-27e7-497b-d3af-ae33c0c80a4f"},"source":["# tokenize and encode sequences in the training set\n","tokens_train = tokenizer.batch_encode_plus(\n","    train_text.tolist(),\n","    max_length = max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n","\n","# tokenize and encode sequences in the validation set\n","tokens_val = tokenizer.batch_encode_plus(\n","    val_text.tolist(),\n","    max_length = max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n","\n","# tokenize and encode sequences in the test set\n","tokens_test = tokenizer.batch_encode_plus(\n","    test_text.tolist(),\n","    max_length = max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"Wsm8bkRZQTw9"},"source":["# Convert Integer Sequences to Tensors"]},{"cell_type":"code","metadata":{"id":"QR-lXwmzQPd6"},"source":["# for train set\n","train_seq = torch.tensor(tokens_train['input_ids'])\n","train_mask = torch.tensor(tokens_train['attention_mask'])\n","train_y = torch.tensor(train_labels.tolist())\n","\n","# for validation set\n","val_seq = torch.tensor(tokens_val['input_ids'])\n","val_mask = torch.tensor(tokens_val['attention_mask'])\n","val_y = torch.tensor(val_labels.tolist())\n","\n","# for test set\n","test_seq = torch.tensor(tokens_test['input_ids'])\n","test_mask = torch.tensor(tokens_test['attention_mask'])\n","test_y = torch.tensor(test_labels.tolist())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ov1cOBlcRLuk"},"source":["# Create DataLoaders"]},{"cell_type":"code","metadata":{"id":"qUy9JKFYQYLp"},"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","#define a batch size\n","batch_size = 32\n","\n","# wrap tensors\n","train_data = TensorDataset(train_seq, train_mask, train_y)\n","\n","# sampler for sampling the data during training\n","train_sampler = RandomSampler(train_data)\n","\n","# dataLoader for train set\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# wrap tensors\n","val_data = TensorDataset(val_seq, val_mask, val_y)\n","\n","# sampler for sampling the data during training\n","val_sampler = SequentialSampler(val_data)\n","\n","# dataLoader for validation set\n","val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K2HZc5ZYRV28"},"source":["# Freeze BERT Parameters"]},{"cell_type":"code","metadata":{"id":"wHZ0MC00RQA_"},"source":["# freeze all the parameters\n","for param in bert.parameters():\n","    param.requires_grad = True\n","# print(bert)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s7ahGBUWRi3X"},"source":["# Define Model Architecture"]},{"cell_type":"code","metadata":{"id":"b3iEtGyYRd0A"},"source":["class BERT_Arch(nn.Module):\n","\n","    def __init__(self, bert):\n","      \n","      super(BERT_Arch, self).__init__()\n","\n","      self.bert = bert \n","      \n","      # dropout layer\n","      self.dropout = nn.Dropout(0.1)\n","      \n","      # relu activation function\n","      self.relu =  nn.ReLU()\n","\n","      # dense layer 1\n","      self.fc1 = nn.Linear(768,512)\n","      \n","      # dense layer 2 (Output layer)\n","      self.fc2 = nn.Linear(512,2)\n","\n","      #softmax activation function\n","      self.softmax = nn.LogSoftmax(dim=1)\n","\n","    #define the forward pass\n","    def forward(self, sent_id, mask):\n","\n","      #pass the inputs to the model  \n","      _, cls_hs = self.bert(sent_id,mask, return_dict=False)\n","      \n","      x = self.fc1(cls_hs)\n","\n","      x = self.relu(x)\n","\n","      x = self.dropout(x)\n","\n","      # output layer\n","      x = self.fc2(x)\n","      \n","      # apply softmax activation\n","      x = self.softmax(x)\n","\n","      return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cBAJJVuJRliv"},"source":["# pass the pre-trained BERT to our define architecture\n","model = BERT_Arch(bert)\n","\n","# push the model to GPU\n","model = model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"taXS0IilRn9J"},"source":["# optimizer from hugging face transformers\n","from transformers import AdamW\n","\n","# define the optimizer\n","optimizer = AdamW(model.parameters(), lr = 2e-5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j9CDpoMQR_rK"},"source":["# Find Class Weights"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"izY5xH5eR7Ur","executionInfo":{"elapsed":14,"status":"ok","timestamp":1621953323331,"user":{"displayName":"ABHISHEK RANJAN","photoUrl":"","userId":"15498253178039914064"},"user_tz":420},"outputId":"edf7b2b7-d244-48b3-8607-6aa2f5e95f41"},"source":["from sklearn.utils.class_weight import compute_class_weight\n","\n","#compute the class weights\n","class_wts = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n","\n","print(class_wts)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[1. 1.]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"r1WvfY2vSGKi"},"source":["# convert class weights to tensor\n","weights= torch.tensor(class_wts,dtype=torch.float)\n","weights = weights.to(device)\n","\n","# loss function\n","cross_entropy  = nn.NLLLoss(weight=weights) \n","\n","# number of training epochs\n","epochs = 10"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"My4CA0qaShLq"},"source":["# Fine-Tune BERT"]},{"cell_type":"code","metadata":{"id":"rskLk8R_SahS"},"source":["# function to train the model\n","def train():\n","  \n","  model.train()\n","\n","  total_loss, total_accuracy = 0, 0\n","  \n","  # empty list to save model predictions\n","  total_preds=[]\n","  \n","  # iterate over batches\n","  for step,batch in enumerate(train_dataloader):\n","    \n","    # progress update after every 50 batches.\n","    if step % 50 == 0 and not step == 0:\n","      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n","\n","    # push the batch to gpu\n","    batch = [r.to(device) for r in batch]\n"," \n","    sent_id, mask, labels = batch\n","\n","    # clear previously calculated gradients \n","    model.zero_grad()        \n","\n","    # get model predictions for the current batch\n","    preds = model(sent_id, mask)\n","\n","    # compute the loss between actual and predicted values\n","    loss = cross_entropy(preds, labels)\n","\n","    # add on to the total loss\n","    total_loss = total_loss + loss.item()\n","\n","    # backward pass to calculate the gradients\n","    loss.backward()\n","\n","    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","    # update parameters\n","    optimizer.step()\n","\n","    # model predictions are stored on GPU. So, push it to CPU\n","    preds=preds.detach().cpu().numpy()\n","\n","    # append the model predictions\n","    total_preds.append(preds)\n","\n","  # compute the training loss of the epoch\n","  avg_loss = total_loss / len(train_dataloader)\n","  \n","  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n","  # reshape the predictions in form of (number of samples, no. of classes)\n","  total_preds  = np.concatenate(total_preds, axis=0)\n","\n","  #returns the loss and predictions\n","  return avg_loss, total_preds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yGXovFDlSxB5"},"source":["# function for evaluating the model\n","def evaluate():\n","  \n","  print(\"\\nEvaluating...\")\n","  \n","  # deactivate dropout layers\n","  model.eval()\n","\n","  total_loss, total_accuracy = 0, 0\n","  \n","  # empty list to save the model predictions\n","  total_preds = []\n","\n","  # iterate over batches\n","  for step,batch in enumerate(val_dataloader):\n","    \n","    # Progress update every 50 batches.\n","    if step % 50 == 0 and not step == 0:\n","      \n","      # Calculate elapsed time in minutes.\n","      # elapsed = format_time(time.time() - t0)\n","            \n","      # Report progress.\n","      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n","\n","    # push the batch to gpu\n","    batch = [t.to(device) for t in batch]\n","\n","    sent_id, mask, labels = batch\n","\n","    # deactivate autograd\n","    with torch.no_grad():\n","      \n","      # model predictions\n","      preds = model(sent_id, mask)\n","\n","      # compute the validation loss between actual and predicted values\n","      loss = cross_entropy(preds,labels)\n","\n","      total_loss = total_loss + loss.item()\n","\n","      preds = preds.detach().cpu().numpy()\n","\n","      total_preds.append(preds)\n","\n","  # compute the validation loss of the epoch\n","  avg_loss = total_loss / len(val_dataloader) \n","\n","  # reshape the predictions in form of (number of samples, no. of classes)\n","  total_preds  = np.concatenate(total_preds, axis=0)\n","\n","  return avg_loss, total_preds"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9KZEgxRRTLXG"},"source":["# Start Model Training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k1USGTntS3TS","executionInfo":{"elapsed":334801,"status":"ok","timestamp":1621953658125,"user":{"displayName":"ABHISHEK RANJAN","photoUrl":"","userId":"15498253178039914064"},"user_tz":420},"outputId":"e5074e5a-f111-4978-ac54-23297cd33762"},"source":["# set initial loss to infinite\n","best_valid_loss = float('inf')\n","\n","# empty lists to store training and validation loss of each epoch\n","train_losses=[]\n","valid_losses=[]\n","\n","#for each epoch\n","epochs=3\n","for epoch in range(epochs):\n","     \n","    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n","    \n","    #train model\n","    train_loss,_ = train()\n","    \n","    #evaluate model\n","    valid_loss, _ = evaluate()\n","    \n","    #save the best model\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'saved_weights.pt')\n","    \n","    # append training and validation loss\n","    train_losses.append(train_loss)\n","    valid_losses.append(valid_loss)\n","    #load weights of best model\n","    path = 'saved_weights.pt'\n","    model.load_state_dict(torch.load(path))\n","    with torch.no_grad():\n","      preds = model(test_seq.to(device), test_mask.to(device))\n","      preds = preds.detach().cpu().numpy()\n","    # model's performance\n","    preds = np.argmax(preds, axis = 1)\n","    print(classification_report(test_y, preds))\n","    \n","    print(f'\\nTraining Loss: {train_loss:.3f}')\n","    print(f'Validation Loss: {valid_loss:.3f}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n"," Epoch 1 / 3\n","  Batch    50  of    157.\n","  Batch   100  of    157.\n","  Batch   150  of    157.\n","\n","Evaluating...\n","  Batch    50  of     57.\n","              precision    recall  f1-score   support\n","\n","           0       0.69      0.77      0.73       900\n","           1       0.74      0.66      0.70       900\n","\n","    accuracy                           0.72      1800\n","   macro avg       0.72      0.72      0.72      1800\n","weighted avg       0.72      0.72      0.72      1800\n","\n","\n","Training Loss: 0.536\n","Validation Loss: 0.573\n","\n"," Epoch 2 / 3\n","  Batch    50  of    157.\n","  Batch   100  of    157.\n","  Batch   150  of    157.\n","\n","Evaluating...\n","  Batch    50  of     57.\n","              precision    recall  f1-score   support\n","\n","           0       0.69      0.77      0.73       900\n","           1       0.74      0.66      0.70       900\n","\n","    accuracy                           0.72      1800\n","   macro avg       0.72      0.72      0.72      1800\n","weighted avg       0.72      0.72      0.72      1800\n","\n","\n","Training Loss: 0.387\n","Validation Loss: 0.608\n","\n"," Epoch 3 / 3\n","  Batch    50  of    157.\n","  Batch   100  of    157.\n","  Batch   150  of    157.\n","\n","Evaluating...\n","  Batch    50  of     57.\n","              precision    recall  f1-score   support\n","\n","           0       0.69      0.77      0.73       900\n","           1       0.74      0.66      0.70       900\n","\n","    accuracy                           0.72      1800\n","   macro avg       0.72      0.72      0.72      1800\n","weighted avg       0.72      0.72      0.72      1800\n","\n","\n","Training Loss: 0.390\n","Validation Loss: 0.574\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_yrhUc9kTI5a"},"source":["# Load Saved Model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OacxUyizS8d1","executionInfo":{"elapsed":1136,"status":"ok","timestamp":1621955149464,"user":{"displayName":"ABHISHEK RANJAN","photoUrl":"","userId":"15498253178039914064"},"user_tz":420},"outputId":"5f86c40e-1da9-47cc-a865-2bb6f1ff73e9"},"source":["#load weights of best model\n","path = 'saved_weights.pt'\n","model.load_state_dict(torch.load(path))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":98}]},{"cell_type":"markdown","metadata":{"id":"x4SVftkkTZXA"},"source":["# Get Predictions for Test Data"]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":470},"id":"NZl0SZmFTRQA","executionInfo":{"elapsed":13300,"status":"error","timestamp":1621955715754,"user":{"displayName":"ABHISHEK RANJAN","photoUrl":"","userId":"15498253178039914064"},"user_tz":420},"outputId":"ab8403d1-9a43-406c-dee3-78719271e8da"},"source":["# get predictions for test data\n","with torch.no_grad():\n","  preds = model(test_seq.to(device), test_mask.to(device))\n","  preds = preds.detach().cpu().numpy()\n","print(preds)\n","p = torch.nn.functional.softmax(preds, dim=1)\n","# df = pd.DataFrame(preds)\n","# df.to_csv(\"bert.csv\")\n","print(p)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[-0.7890497  -0.60564244]\n"," [-0.26606497 -1.4540995 ]\n"," [-0.2394344  -1.5468054 ]\n"," ...\n"," [-0.5602411  -0.8464649 ]\n"," [-3.0385964  -0.04908742]\n"," [-3.1298473  -0.04470916]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pGKsqe4uGZHw"},"source":["import scipy.special.softmax as softmax\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ms1ObHZxTYSI"},"source":["# model's performance\n","preds = np.argmax(preds, axis = 1)\n","# print(classification_report(test_y, preds))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YqzLS7rHTp4T"},"source":["# confusion matrix\n","# # pd.crosstab(test_y, preds)\n","# from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","\n","# print(accuracy_score(test_y, preds))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jpX1uTwjUPY6"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BY2P7H1W2Wo4"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XwdIKhqP2B3Q"},"source":["# SVC"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aBPgaC832BhW","executionInfo":{"elapsed":2771,"status":"ok","timestamp":1621953673366,"user":{"displayName":"ABHISHEK RANJAN","photoUrl":"","userId":"15498253178039914064"},"user_tz":420},"outputId":"2c668676-5eae-49e3-b430-9be598dc3d7c"},"source":["import pandas as pd\n","import re\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.corpus import stopwords\n","import re\n","import string\n","import numpy as np\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import TweetTokenizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from tqdm.auto import tqdm\n","!pip install demoji\n","import demoji\n","demoji.download_codes()\n","nltk.download('stopwords') \n","nltk.download('wordnet')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: demoji in /usr/local/lib/python3.7/dist-packages (0.4.0)\n","Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.7/dist-packages (from demoji) (2.23.0)\n","Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from demoji) (0.4.4)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (1.24.3)\n","Downloading emoji data ...\n","... OK (Got response in 0.21 seconds)\n","Writing emoji data to /root/.demoji/codes.json ...\n","... OK\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":87}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rK8SHhXg2Beb","executionInfo":{"elapsed":502,"status":"ok","timestamp":1621953673860,"user":{"displayName":"ABHISHEK RANJAN","photoUrl":"","userId":"15498253178039914064"},"user_tz":420},"outputId":"024205dc-8f14-4a5e-fc40-658288d6e932"},"source":["df['context_resp']=df['label']\n","for i in range(5000):\n","  df['context_resp'][i]=''.join(df['context'][i]) + df['response'][i]\n","\n","print(df['context'][12])\n","print(df['response'][12])\n","print(df['context_resp'][12])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  This is separate from the ipykernel package so we can avoid doing imports until\n"],"name":"stderr"},{"output_type":"stream","text":["['Matt here . Rarely comment on the president , but having someone who is not Jewish tell Jews what they should or shouldn ’ t think is beyond the pale and it should not go unchallenged . <URL>', '@USER It seems there is a long history of non-Jews telling Jews what to think .']\n","@USER @USER Hey , but what do they have to lose ? Asking for some black friends .\n","Matt here . Rarely comment on the president , but having someone who is not Jewish tell Jews what they should or shouldn ’ t think is beyond the pale and it should not go unchallenged . <URL>@USER It seems there is a long history of non-Jews telling Jews what to think .@USER @USER Hey , but what do they have to lose ? Asking for some black friends .\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2ClztRix2BcP"},"source":["def preprocess_text(text):\n","    # Tokenise words while ignoring punctuation(https://www.nltk.org/_modules/nltk/tokenize/regexp.html)\n","    text = re.sub(r\"@USER\",\"\", text)\n","    text = re.sub(r'<URL>','',text)\n","    text = demoji.replace_with_desc(text)\n","    text = re.sub(r':','',text)\n","    # tokeniser = RegexpTokenizer(r'\\s+', gaps=True)\n","    # tokeniser = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n","    tokeniser = RegexpTokenizer(r'\\w+')\n","    tokens = tokeniser.tokenize(text)\n","\n","    # Lowercase and lemmatise \n","    lemmatiser = WordNetLemmatizer()\n","    lemmas = [lemmatiser.lemmatize(token.lower(), pos='v') for token in tokens]\n","    \n","    # Remove stopwords\n","    keywords= [lemma for lemma in lemmas if lemma not in stopwords.words('english')]\n","    \n","    # stemming\n","    ps=PorterStemmer()\n","    keywords=[ps.stem(x) for x in keywords]\n","    return keywords\n","\n","def preprocess_text2(tweet):\n","    \"\"\"Process tweet function.\n","    Input:\n","        tweet: a string containing a tweet\n","    Output:\n","        tweets_clean: a list of words containing the processed tweet\n","\n","    \"\"\"\n","    stemmer = PorterStemmer()\n","    stopwords_english = stopwords.words('english')\n","    # remove stock market tickers like $GE\n","    tweet = re.sub(r'\\$\\w*', '', tweet)\n","    # remove old style retweet text \"RT\"\n","    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n","    # remove hyperlinks\n","    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n","    # remove hashtags\n","    # only removing the hash # sign from the word\n","    tweet = re.sub(r'#', '', tweet)\n","    # tokenize tweets\n","    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n","                               reduce_len=True)\n","    tweet_tokens = tokenizer.tokenize(tweet)\n","\n","    tweets_clean = []\n","    for word in tweet_tokens:\n","        if (word not in stopwords_english and  # remove stopwords\n","                word not in string.punctuation):  # remove punctuation (optional)\n","            # tweets_clean.append(word)\n","            stem_word = stemmer.stem(word)  # stemming word\n","            tweets_clean.append(stem_word)\n","\n","    return tweets_clean"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cVk70anG2Qog","executionInfo":{"elapsed":554,"status":"ok","timestamp":1621953674389,"user":{"displayName":"ABHISHEK RANJAN","photoUrl":"","userId":"15498253178039914064"},"user_tz":420},"outputId":"2e4b8eb3-681b-4417-a083-ef64d7d8ecce"},"source":["for i in range(5):\n","  print(df['response'][i])\n","  print(preprocess_text(df['response'][i]))\n","  print(preprocess_text2(df['response'][i]))\n","  print()\n","  print(df['context_resp'][i])\n","  print(preprocess_text(df['context_resp'][i]))\n","  print(preprocess_text2(df['context_resp'][i]))\n","  print()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["@USER @USER @USER I don't get this .. obviously you do care or you would've moved right along .. instead you decided to care and troll her ..\n","['get', 'obvious', 'care', 'would', 'move', 'right', 'along', 'instead', 'decid', 'care', 'troll']\n","['get', '..', 'obvious', 'care', \"would'v\", 'move', 'right', 'along', '..', 'instead', 'decid', 'care', 'troll', '..']\n","\n","A minor child deserves privacy and should be kept out of politics . Pamela Karlan , you should be ashamed of your very angry and obviously biased public pandering , and using a child to do it .@USER If your child isn't named Barron ... #BeBest Melania couldn't care less . Fact . 💯@USER @USER @USER I don't get this .. obviously you do care or you would've moved right along .. instead you decided to care and troll her ..\n","['minor', 'child', 'deserv', 'privaci', 'keep', 'polit', 'pamela', 'karlan', 'asham', 'angri', 'obvious', 'bia', 'public', 'pander', 'use', 'child', 'child', 'name', 'barron', 'bebest', 'melania', 'care', 'less', 'fact', 'hundr', 'point', 'get', 'obvious', 'care', 'would', 'move', 'right', 'along', 'instead', 'decid', 'care', 'troll']\n","['minor', 'child', 'deserv', 'privaci', 'kept', 'polit', 'pamela', 'karlan', 'asham', 'angri', 'obvious', 'bias', 'public', 'pander', 'use', 'child', 'child', 'name', 'barron', '...', 'bebest', 'melania', 'care', 'less', 'fact', '💯', 'get', '..', 'obvious', 'care', \"would'v\", 'move', 'right', 'along', '..', 'instead', 'decid', 'care', 'troll', '..']\n","\n","@USER @USER trying to protest about . Talking about him and his labels and they label themselves WTF does that make em ?\n","['tri', 'protest', 'talk', 'label', 'label', 'wtf', 'make', 'em']\n","['tri', 'protest', 'talk', 'label', 'label', 'wtf', 'make', 'em']\n","\n","@USER @USER Why is he a loser ? He's just a Press Secretary@USER @USER having to make up excuses of why your crowd was small .@USER @USER trying to protest about . Talking about him and his labels and they label themselves WTF does that make em ?\n","['loser', 'press', 'secretari', 'make', 'excus', 'crowd', 'small', 'tri', 'protest', 'talk', 'label', 'label', 'wtf', 'make', 'em']\n","['loser', \"he'\", 'press', 'secretari', '@user', 'make', 'excus', 'crowd', 'small', 'tri', 'protest', 'talk', 'label', 'label', 'wtf', 'make', 'em']\n","\n","@USER @USER @USER He makes an insane about of money from the MOVIES , Einstein ! #LearnHowTheSystemWorks\n","['make', 'insan', 'money', 'movi', 'einstein', 'learnhowthesystemwork']\n","['make', 'insan', 'money', 'movi', 'einstein', 'learnhowthesystemwork']\n","\n","Donald J . Trump is guilty as charged . The evidence is clear . If your Senator votes to acquit , remember him / her at the ballot box .@USER I ’ ll remember to not support you at the box office .@USER @USER @USER He makes an insane about of money from the MOVIES , Einstein ! #LearnHowTheSystemWorks\n","['donald', 'j', 'trump', 'guilti', 'charg', 'evid', 'clear', 'senat', 'vote', 'acquit', 'rememb', 'ballot', 'box', 'rememb', 'support', 'box', 'offic', 'make', 'insan', 'money', 'movi', 'einstein', 'learnhowthesystemwork']\n","['donald', 'j', 'trump', 'guilti', 'charg', 'evid', 'clear', 'senat', 'vote', 'acquit', 'rememb', 'ballot', 'box', '’', 'rememb', 'support', 'box', 'offic', 'make', 'insan', 'money', 'movi', 'einstein', 'learnhowthesystemwork']\n","\n","@USER @USER Meanwhile Trump won't even release his SAT scores and his Wharton professors said he was the dumbest student they've ever taught\n","['meanwhil', 'trump', 'win', 'even', 'releas', 'sit', 'score', 'wharton', 'professor', 'say', 'dumbest', 'student', 'ever', 'teach']\n","['meanwhil', 'trump', 'even', 'releas', 'sat', 'score', 'wharton', 'professor', 'said', 'dumbest', 'student', \"they'v\", 'ever', 'taught']\n","\n","Jamie Raskin tanked Doug Collins . Collins looks stupid . <URL>@USER But not half as stupid as Schiff looks . People's looks are what nature creates . Abilities of people are what they create for themselves . Just for looks , Lincoln was not tops but with his education and understanding made him great .@USER @USER Meanwhile Trump won't even release his SAT scores and his Wharton professors said he was the dumbest student they've ever taught\n","['jami', 'raskin', 'tank', 'doug', 'collin', 'collin', 'look', 'stupid', 'half', 'stupid', 'schiff', 'look', 'peopl', 'look', 'natur', 'creat', 'abil', 'peopl', 'creat', 'look', 'lincoln', 'top', 'educ', 'understand', 'make', 'great', 'meanwhil', 'trump', 'win', 'even', 'releas', 'sit', 'score', 'wharton', 'professor', 'say', 'dumbest', 'student', 'ever', 'teach']\n","['jami', 'raskin', 'tank', 'doug', 'collin', 'collin', 'look', 'stupid', '<url>', 'half', 'stupid', 'schiff', 'look', \"people'\", 'look', 'natur', 'creat', 'abil', 'peopl', 'creat', 'look', 'lincoln', 'top', 'educ', 'understand', 'made', 'great', 'meanwhil', 'trump', 'even', 'releas', 'sat', 'score', 'wharton', 'professor', 'said', 'dumbest', 'student', \"they'v\", 'ever', 'taught']\n","\n","@USER @USER Pretty Sure the Anti-Lincoln Crowd Claimed That \" Democracy Was on the Ballot \" in 1860 , too . They Thought Lincoln Was \" Authoritarian \" . #GOP #PartyOfLincoln #Democrats\n","['pretti', 'sure', 'anti', 'lincoln', 'crowd', 'claim', 'democraci', 'ballot', '1860', 'think', 'lincoln', 'authoritarian', 'gop', 'partyoflincoln', 'democrat']\n","['pretti', 'sure', 'anti-lincoln', 'crowd', 'claim', 'democraci', 'ballot', '1860', 'thought', 'lincoln', 'authoritarian', 'gop', 'partyoflincoln', 'democrat']\n","\n","Man ... y ’ all gone “ both sides ” the apocalypse one day . <URL>@USER They already did . Obama said many times during the 2016 campaign that democracy was on the ballot . Then . Hillary warned you , so did every single expert in authoritarianism . The media , like millions of Americans , refused to believe them .@USER @USER Pretty Sure the Anti-Lincoln Crowd Claimed That \" Democracy Was on the Ballot \" in 1860 , too . They Thought Lincoln Was \" Authoritarian \" . #GOP #PartyOfLincoln #Democrats\n","['man', 'go', 'side', 'apocalyps', 'one', 'day', 'alreadi', 'obama', 'say', 'mani', 'time', '2016', 'campaign', 'democraci', 'ballot', 'hillari', 'warn', 'everi', 'singl', 'expert', 'authoritarian', 'media', 'like', 'million', 'american', 'refus', 'believ', 'pretti', 'sure', 'anti', 'lincoln', 'crowd', 'claim', 'democraci', 'ballot', '1860', 'think', 'lincoln', 'authoritarian', 'gop', 'partyoflincoln', 'democrat']\n","['man', '...', '’', 'gone', '“', 'side', '”', 'apocalyps', 'one', 'day', '<url>', 'alreadi', 'obama', 'said', 'mani', 'time', '2016', 'campaign', 'democraci', 'ballot', 'hillari', 'warn', 'everi', 'singl', 'expert', 'authoritarian', 'media', 'like', 'million', 'american', 'refus', 'believ', 'pretti', 'sure', 'anti-lincoln', 'crowd', 'claim', 'democraci', 'ballot', '1860', 'thought', 'lincoln', 'authoritarian', 'gop', 'partyoflincoln', 'democrat']\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xkR2-1Yu2QqG"},"source":["from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import confusion_matrix, accuracy_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rqA5WFn-2Qr5","executionInfo":{"elapsed":10,"status":"ok","timestamp":1621953674390,"user":{"displayName":"ABHISHEK RANJAN","photoUrl":"","userId":"15498253178039914064"},"user_tz":420},"outputId":"609f8bf5-1b87-4753-a2c1-06f2a528dfbf"},"source":["print(df['label'].value_counts())\n","print(df1['label'].value_counts())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["NOT_SARCASM    2500\n","SARCASM        2500\n","Name: label, dtype: int64\n","SARCASM        900\n","NOT_SARCASM    900\n","Name: label, dtype: int64\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lLTXOOUr2Qt9"},"source":["from sklearn.utils import shuffle\n","X_train, Y_train = shuffle(df['response'],df['label'],random_state=123)\n","X_test_svm, Y_test = df1['response'],df1['label']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"meKQnipB6bbA","executionInfo":{"elapsed":2777,"status":"ok","timestamp":1621953677163,"user":{"displayName":"ABHISHEK RANJAN","photoUrl":"","userId":"15498253178039914064"},"user_tz":420},"outputId":"6eb438ec-fd88-48ff-f762-9de87d2c8aae"},"source":["pip install -U sentence-transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: sentence-transformers in /usr/local/lib/python3.7/dist-packages (1.2.0)\n","Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n","Requirement already satisfied, skipping upgrade: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.9.1+cu101)\n","Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.41.1)\n","Requirement already satisfied, skipping upgrade: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.8.1+cu101)\n","Requirement already satisfied, skipping upgrade: transformers<5.0.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.6.1)\n","Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n","Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n","Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n","Requirement already satisfied, skipping upgrade: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.95)\n","Requirement already satisfied, skipping upgrade: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n","Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n","Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2019.12.20)\n","Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.23.0)\n","Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n","Requirement already satisfied, skipping upgrade: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.10.3)\n","Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.9)\n","Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (4.0.1)\n","Requirement already satisfied, skipping upgrade: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.0.8)\n","Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.0.45)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n","Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (1.0.1)\n","Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.4)\n","Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n","Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.24.3)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.12.5)\n","Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n","Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.1)\n","Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (8.0.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":340},"id":"rAql1cK02Qxb","executionInfo":{"elapsed":33,"status":"error","timestamp":1621953677170,"user":{"displayName":"ABHISHEK RANJAN","photoUrl":"","userId":"15498253178039914064"},"user_tz":420},"outputId":"585d7f1d-0077-45d7-b244-13e76ee330c8"},"source":["from sentence_transformers import SentenceTransformer\n","# sbert_model = SentenceTransformer('stsb-mpnet-base-v2')\n","sbert_model = SentenceTransformer('paraphrase-distilroberta-base-v1')"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-95-e656d931f788>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# sbert_model = SentenceTransformer('stsb-mpnet-base-v2')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msbert_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'paraphrase-distilroberta-base-v1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sentence_transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"1.2.0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0m__DOWNLOAD_SERVER__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'http://sbert.net/models/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentencesDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParallelSentencesDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mLoggingHandler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoggingHandler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mSentenceTransformer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sentence_transformers/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mDenoisingAutoEncoderDataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDenoisingAutoEncoderDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mNoDuplicatesDataLoader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNoDuplicatesDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mParallelSentencesDataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParallelSentencesDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mSentencesDataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentencesDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mSentenceLabelDataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceLabelDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sentence_transformers/datasets/ParallelSentencesDataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInputExample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceEvaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimport_from_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_to_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_get\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPooling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sentence_transformers/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mTransformer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mAsym\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAsym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mBoW\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBoW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mCNN\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2707\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__version__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2709\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2711\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LazyModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_import_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1820\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1822\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1823\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1824\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module {self.__name__} has no attribute {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1819\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1820\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1821\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1822\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1823\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/auto/__init__.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LazyModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_import_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmbart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_mbart\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMBartTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmbart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_mbart50\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMBart50Tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmt5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMT5Tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpegasus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_pegasus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPegasusTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_reformer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReformerTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/mt5/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"MT5Tokenizer\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mMT5Tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"MT5TokenizerFast\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mMT5TokenizerFast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'MT5Tokenizer' is not defined"]}]},{"cell_type":"code","metadata":{"id":"LTHd0ki82fZH"},"source":["# sgd_clf = SGDClassifier(random_state=123)\n","from tqdm.auto import tqdm\n","import numpy as np\n","query = \"I had pizza and pasta\"\n","# sentence_embeddings = sbert_model.encode(query) \n","sentence_embeddings = [sbert_model.encode(i.lower()) for i in tqdm(X_train)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VU2D5fyk2fbu"},"source":["query = \"I had pizza and pasta\"\n","# sentence_embeddings = sbert_model.encode(query) \n","test_embeddings = [sbert_model.encode(i.lower()) for i in tqdm(X_test_svm)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q5CAynIK2fdy"},"source":["sentence_embeddings=np.array(sentence_embeddings)\n","test_embeddings=np.array(test_embeddings)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-xiKHg1d2ffz"},"source":["# X_train2, X_test2, Y_train2, Y_test2=train_test_split(sentence_embeddings,Y_train,random_state=42)\n","X_train2, X_test2, Y_train2, Y_test2=sentence_embeddings, test_embeddings, Y_train, Y_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iiqWcnVt2fjH"},"source":["X_test_=test_embeddings"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"idgO-2eA2qVo"},"source":["from sklearn.svm import SVC\n","model= SVC(random_state=24,probability=True)\n","\n","model.fit(sentence_embeddings, Y_train) # fit the model\n","y_pred= model.predict(test_embeddings) # then predict on the test set\n","accuracy= accuracy_score(Y_test, y_pred) # this gives us how often the algorithm predicted correctly\n","# clf_report= classification_report(Y_test, y_pred) # with the report, we have a bigger picture, with precision and recall for each class\n","print(f\"The accuracy of model {type(model).__name__} is {accuracy:.2f}\")\n","# print(clf_report)\n","print(\"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"haNY9i_W2qXj"},"source":["pr=model.predict_proba(test_embeddings)\n","pr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ilrwnS1T2qZW"},"source":["pr2=np.subtract(pr,1)\n","pr3=np.multiply(pr2,-1)\n","pr3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i7y3Ylfd2qbQ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LxUDAA_22qdN"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GZOjqme22qgg"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kpc4aBeP3N50"},"source":["#Ensemble"]},{"cell_type":"code","metadata":{"id":"xqnX6PX93Q7Y"},"source":["yhats = [preds,pr3]\n","import numpy as np\n","yhats = np.array(yhats)\n","# sum across ensemble members\n","summed = np.sum(yhats, axis=0)\n","# argmax across classes\n","result = np.argmax(summed, axis=1)\n","# return result\n","print(result)\n","print(testing)\n","# # evaluate a specific number of members in an ensemble\n","# def evaluate_n_members(members, n_members, testX, testy):\n","# \t# select a subset of members\n","# \tsubset = members[:n_members]\n","# \tprint(len(subset))\n","# \t# make prediction\n","# \tyhat = ensemble_predictions(subset, testX)\n","# \t# calculate accuracy\n","accuracy_score(result, testing)"],"execution_count":null,"outputs":[]}]}