{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Fine-Tuning BERT.ipynb","provenance":[{"file_id":"https://github.com/prateekjoshi565/Fine-Tuning-BERT/blob/master/Fine_Tuning_BERT_for_Spam_Classification.ipynb","timestamp":1619947106006}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"85a59527e6bd488aa2a2366e49be0957":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_de3fdc4378264859b7d7ecbaf38c80ed","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e59782884f4d418d987c7d4781e348b6","IPY_MODEL_4cb6f3479b9a4e0b8477ce673c07ec6f"]}},"de3fdc4378264859b7d7ecbaf38c80ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e59782884f4d418d987c7d4781e348b6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_bee27b99410b4195b9b1dc00b86f4173","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":570,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":570,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_341d670696c546218df9c78513a31ca1"}},"4cb6f3479b9a4e0b8477ce673c07ec6f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_189490762ddd45a4aa8ef28731d06e0b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 570/570 [03:21&lt;00:00, 2.83B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7965dd6cba38475a8431c97e8717ad4d"}},"bee27b99410b4195b9b1dc00b86f4173":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"341d670696c546218df9c78513a31ca1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"189490762ddd45a4aa8ef28731d06e0b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7965dd6cba38475a8431c97e8717ad4d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4d49a7a21ac14321afe3b089c22dc749":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_1dc843e44b9f4d4dab05122e5673c588","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d5bddf45c8124ffc977a41562cac24e7","IPY_MODEL_50a75bfaa6d244cc85227a824589964e"]}},"1dc843e44b9f4d4dab05122e5673c588":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d5bddf45c8124ffc977a41562cac24e7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_16ec92b8296e40cfa16ae5f010b1e2c1","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":440473133,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":440473133,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b047e3aec19a4992b587952e20f98c30"}},"50a75bfaa6d244cc85227a824589964e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_50f352ce7b9343579387c041654d8326","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 440M/440M [00:09&lt;00:00, 48.8MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f1fa1e1821c448328f1f218a4fb181c3"}},"16ec92b8296e40cfa16ae5f010b1e2c1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b047e3aec19a4992b587952e20f98c30":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"50f352ce7b9343579387c041654d8326":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f1fa1e1821c448328f1f218a4fb181c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c867bb5773c5426f9fa204372d03f663":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_685a306e54d64c6dbc487dbffcbec112","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6e7787fb57234235ba9bf194d224ecd0","IPY_MODEL_b9bc87ba210d47efa30498029d718c93"]}},"685a306e54d64c6dbc487dbffcbec112":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6e7787fb57234235ba9bf194d224ecd0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7432a13b28894a759f9d003f838d9e47","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_30bff3e16a2e4bb2b23302ebf07a21d8"}},"b9bc87ba210d47efa30498029d718c93":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_70c17c71599e46629e05d9df9c2fc414","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:02&lt;00:00, 110kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cff767bca6cc4b528a51c238157c241b"}},"7432a13b28894a759f9d003f838d9e47":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"30bff3e16a2e4bb2b23302ebf07a21d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"70c17c71599e46629e05d9df9c2fc414":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"cff767bca6cc4b528a51c238157c241b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8498210b612140c59e7beec0eb178eef":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c7a2413b710c4f73aedea1f36076453a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c838008d0004485b9dc37bee95abf060","IPY_MODEL_cb13a278061e4c6cb352501aa9d401ab"]}},"c7a2413b710c4f73aedea1f36076453a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c838008d0004485b9dc37bee95abf060":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_41ade49ffbab4d64b4683fac0a0ae339","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":466062,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":466062,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0b1b18b0fdab4ae8bfbc1fe4799c8c8a"}},"cb13a278061e4c6cb352501aa9d401ab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_45c6e9555139453b975c62192874252b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 466k/466k [00:03&lt;00:00, 143kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_206b3776cbd444619198dc798d3800c9"}},"41ade49ffbab4d64b4683fac0a0ae339":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0b1b18b0fdab4ae8bfbc1fe4799c8c8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"45c6e9555139453b975c62192874252b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"206b3776cbd444619198dc798d3800c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"884fb578a00d4bc89625cefdf6ee61d5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7d53b250f91c47e3aedb7392f2af2d35","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_5e3ab4ca2dc14426b270ad0bb812d302","IPY_MODEL_1f2e0f99f6d44bb3bbac6ae01dfbdffc"]}},"7d53b250f91c47e3aedb7392f2af2d35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5e3ab4ca2dc14426b270ad0bb812d302":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a07f3e0d66f04a66aa93f45746fe97b6","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":28,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":28,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_05a5d9235c6d4d65bdc08cb44565c88b"}},"1f2e0f99f6d44bb3bbac6ae01dfbdffc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_859c8392e8304668b07bf56b3a0daa0e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 28.0/28.0 [00:00&lt;00:00, 224B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_24ccf6608df1431f96b8b8359f802ffd"}},"a07f3e0d66f04a66aa93f45746fe97b6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"05a5d9235c6d4d65bdc08cb44565c88b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"859c8392e8304668b07bf56b3a0daa0e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"24ccf6608df1431f96b8b8359f802ffd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"OFOTiqrtNvyy"},"source":["# Install Transformers Library"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7sYuExU3YiR4","executionInfo":{"status":"ok","timestamp":1621159082317,"user_tz":420,"elapsed":42325,"user":{"displayName":"ABHISHEK RANJAN","photoUrl":"","userId":"15498253178039914064"}},"outputId":"e924c025-e39a-4ccb-fa37-c1e2e126fab7"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1hkhc10wNrGt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621159049503,"user_tz":420,"elapsed":9526,"user":{"displayName":"ABHISHEK RANJAN","photoUrl":"","userId":"15498253178039914064"}},"outputId":"a152062a-a965-43da-8f75-e32335d87b9e"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/9e/5b80becd952d5f7250eaf8fc64b957077b12ccfe73e9c03d37146ab29712/transformers-4.6.0-py3-none-any.whl (2.3MB)\n","\u001b[K     |████████████████████████████████| 2.3MB 4.3MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Collecting huggingface-hub==0.0.8\n","  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 49.4MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 49.2MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (8.0.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Installing collected packages: huggingface-hub, tokenizers, sacremoses, transformers\n","Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.6.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"x4giRzM7NtHJ"},"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import transformers\n","from transformers import AutoModel, BertTokenizerFast\n","\n","# specify GPU\n","device = torch.device(\"cuda\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kKd-Tj3hOMsZ"},"source":["# Load Dataset"]},{"cell_type":"code","metadata":{"id":"cwJrQFQgN_BE","colab":{"base_uri":"https://localhost:8080/","height":195},"executionInfo":{"status":"ok","timestamp":1621159086311,"user_tz":420,"elapsed":46310,"user":{"displayName":"ABHISHEK RANJAN","photoUrl":"","userId":"15498253178039914064"}},"outputId":"7483a4b6-0fda-43a1-90a1-0c0d6e2acf05"},"source":["df = pd.read_json('/content/drive/MyDrive/sercasm/twitter/sarcasm_detection_shared_task_twitter_training.jsonl',lines=True)\n","df1 = pd.read_json('/content/drive/MyDrive/sercasm/twitter/sarcasm_detection_shared_task_twitter_testing.jsonl',lines=True)\n","\n","df['labels'] = df['label'].apply(lambda x: ['SARCASM', 'NOT_SARCASM'].index(x))\n","df1['labels'] = df1['label'].apply(lambda x: ['SARCASM', 'NOT_SARCASM'].index(x))\n","df1.tail()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>context</th>\n","      <th>response</th>\n","      <th>id</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1795</th>\n","      <td>NOT_SARCASM</td>\n","      <td>[I have been a business customer of MWeb @USER...</td>\n","      <td>@USER @USER @USER is definitely the best out t...</td>\n","      <td>twitter_1796</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1796</th>\n","      <td>SARCASM</td>\n","      <td>[A woman refuses to have her temperature taken...</td>\n","      <td>@USER @USER Ye let her out run wild and infect...</td>\n","      <td>twitter_1797</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1797</th>\n","      <td>SARCASM</td>\n","      <td>[The reason big government wants @USER out is ...</td>\n","      <td>@USER @USER @USER Thanks for that , I would ha...</td>\n","      <td>twitter_1798</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1798</th>\n","      <td>NOT_SARCASM</td>\n","      <td>[Happy #musicmonday and #thanks for #all your ...</td>\n","      <td>@USER @USER @USER Yes also #found this on #new...</td>\n","      <td>twitter_1799</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1799</th>\n","      <td>NOT_SARCASM</td>\n","      <td>[Not long wrapped on the amazing #January22nd ...</td>\n","      <td>@USER @USER @USER you still need to send the l...</td>\n","      <td>twitter_1800</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            label  ... labels\n","1795  NOT_SARCASM  ...      1\n","1796      SARCASM  ...      0\n","1797      SARCASM  ...      0\n","1798  NOT_SARCASM  ...      1\n","1799  NOT_SARCASM  ...      1\n","\n","[5 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"U7WBye79ZKoO"},"source":["df[\"tweets\"]=df['response']\n","df1[\"tweets\"]=df1['response']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fzPPOrVQWiW5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621159086312,"user_tz":420,"elapsed":46300,"user":{"displayName":"ABHISHEK RANJAN","photoUrl":"","userId":"15498253178039914064"}},"outputId":"364cd8d5-d403-4957-da90-ca59ee0cce49"},"source":["df.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5000, 5)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"676DPU1BOPdp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621159086313,"user_tz":420,"elapsed":46293,"user":{"displayName":"ABHISHEK RANJAN","photoUrl":"","userId":"15498253178039914064"}},"outputId":"64edda46-b6e8-4259-db50-6d65e775a701"},"source":["# check class distribution\n","df['labels'].value_counts(normalize = True)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1    0.5\n","0    0.5\n","Name: labels, dtype: float64"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"MKfWnApvOoE7"},"source":["# Split train dataset into train, validation and test sets"]},{"cell_type":"code","metadata":{"id":"mfhSPF5jOWb7"},"source":["\n","X_s = df['tweets'].values\n","y_s = df['labels'].values\n","\n","Xt_s = df1['tweets'].values\n","yt_s = df1['labels'].values\n","\n","train_text, temp_text, train_labels, temp_labels = X_s,Xt_s,y_s,yt_s\n","\n","# we will use temp_text and temp_labels to create validation and test set\n","val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n","                                                                random_state=2018, \n","                                                                test_size=0.5, \n","                                                                stratify=temp_labels)\n","val_text, test_text, val_labels, test_labels=temp_text, temp_text, temp_labels, temp_labels\n","\n","# X_train, X_test, y_train, y_test = train_test_split(X_s, y_s, test_size=0.2, random_state=42)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W1jEFb39cCjk","executionInfo":{"status":"ok","timestamp":1621159091782,"user_tz":420,"elapsed":51751,"user":{"displayName":"ABHISHEK RANJAN","photoUrl":"","userId":"15498253178039914064"}},"outputId":"26299a09-4ee4-4fab-e6b8-11e940b44f6e"},"source":["# pip install -U sentence-transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting sentence-transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/9d/abacb6f7bb63df39285c55bb51b6403a7fd93ac2aea48b01f6215175446c/sentence-transformers-1.1.1.tar.gz (81kB)\n","\r\u001b[K     |████                            | 10kB 19.9MB/s eta 0:00:01\r\u001b[K     |████████                        | 20kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 30kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 40kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 51kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 61kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 71kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 3.6MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: transformers<5.0.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.6.0)\n","Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.41.1)\n","Requirement already satisfied, skipping upgrade: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.8.1+cu101)\n","Requirement already satisfied, skipping upgrade: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.9.1+cu101)\n","Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n","Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n","Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n","Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n","\u001b[K     |████████████████████████████████| 1.2MB 23.8MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.9)\n","Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n","Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2019.12.20)\n","Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.0.45)\n","Requirement already satisfied, skipping upgrade: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.0.8)\n","Requirement already satisfied, skipping upgrade: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.10.2)\n","Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.23.0)\n","Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (4.0.1)\n","Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n","Requirement already satisfied, skipping upgrade: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n","Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (1.0.1)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n","Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n","Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (8.0.0)\n","Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n","Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.4)\n","Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.24.3)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.12.5)\n","Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.1)\n","Building wheels for collected packages: sentence-transformers\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence-transformers: filename=sentence_transformers-1.1.1-cp37-none-any.whl size=123338 sha256=ba63c6af3753dd393cd905f1061e96510b760d61774e049571f4afc736e7acd0\n","  Stored in directory: /root/.cache/pip/wheels/5e/89/29/45e45adc162b50f97f71801e8b07947c9cfe2b3ae7dbf37896\n","Successfully built sentence-transformers\n","Installing collected packages: sentencepiece, sentence-transformers\n","Successfully installed sentence-transformers-1.1.1 sentencepiece-0.1.95\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"n7hsdLoCO7uB"},"source":["# Import BERT Model and BERT Tokenizer"]},{"cell_type":"code","metadata":{"id":"S1kY3gZjO2RE","colab":{"base_uri":"https://localhost:8080/","height":333,"referenced_widgets":["85a59527e6bd488aa2a2366e49be0957","de3fdc4378264859b7d7ecbaf38c80ed","e59782884f4d418d987c7d4781e348b6","4cb6f3479b9a4e0b8477ce673c07ec6f","bee27b99410b4195b9b1dc00b86f4173","341d670696c546218df9c78513a31ca1","189490762ddd45a4aa8ef28731d06e0b","7965dd6cba38475a8431c97e8717ad4d","4d49a7a21ac14321afe3b089c22dc749","1dc843e44b9f4d4dab05122e5673c588","d5bddf45c8124ffc977a41562cac24e7","50a75bfaa6d244cc85227a824589964e","16ec92b8296e40cfa16ae5f010b1e2c1","b047e3aec19a4992b587952e20f98c30","50f352ce7b9343579387c041654d8326","f1fa1e1821c448328f1f218a4fb181c3","c867bb5773c5426f9fa204372d03f663","685a306e54d64c6dbc487dbffcbec112","6e7787fb57234235ba9bf194d224ecd0","b9bc87ba210d47efa30498029d718c93","7432a13b28894a759f9d003f838d9e47","30bff3e16a2e4bb2b23302ebf07a21d8","70c17c71599e46629e05d9df9c2fc414","cff767bca6cc4b528a51c238157c241b","8498210b612140c59e7beec0eb178eef","c7a2413b710c4f73aedea1f36076453a","c838008d0004485b9dc37bee95abf060","cb13a278061e4c6cb352501aa9d401ab","41ade49ffbab4d64b4683fac0a0ae339","0b1b18b0fdab4ae8bfbc1fe4799c8c8a","45c6e9555139453b975c62192874252b","206b3776cbd444619198dc798d3800c9","884fb578a00d4bc89625cefdf6ee61d5","7d53b250f91c47e3aedb7392f2af2d35","5e3ab4ca2dc14426b270ad0bb812d302","1f2e0f99f6d44bb3bbac6ae01dfbdffc","a07f3e0d66f04a66aa93f45746fe97b6","05a5d9235c6d4d65bdc08cb44565c88b","859c8392e8304668b07bf56b3a0daa0e","24ccf6608df1431f96b8b8359f802ffd"]},"executionInfo":{"status":"ok","timestamp":1621159109222,"user_tz":420,"elapsed":69184,"user":{"displayName":"ABHISHEK RANJAN","photoUrl":"","userId":"15498253178039914064"}},"outputId":"61a87973-7f88-4843-89bc-b7965f3367d1"},"source":["# import BERT-base pretrained model\n","bert = AutoModel.from_pretrained('bert-base-uncased')\n","# bert = AutoModel.from_pretrained('bert-large-cased')\n","# Load the BERT tokenizer\n","tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"85a59527e6bd488aa2a2366e49be0957","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4d49a7a21ac14321afe3b089c22dc749","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c867bb5773c5426f9fa204372d03f663","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8498210b612140c59e7beec0eb178eef","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"884fb578a00d4bc89625cefdf6ee61d5","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_zOKeOMeO-DT"},"source":["# sample data\n","text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n","# encode text\n","sent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oAH73n39PHLw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621159109224,"user_tz":420,"elapsed":69172,"user":{"displayName":"ABHISHEK RANJAN","photoUrl":"","userId":"15498253178039914064"}},"outputId":"a14818d9-8386-4ccc-e00f-0239039c742d"},"source":["# output\n","print(sent_id)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'input_ids': [[101, 2023, 2003, 1037, 14324, 2944, 14924, 4818, 102, 0], [101, 2057, 2097, 2986, 1011, 8694, 1037, 14324, 2944, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8wIYaWI_Prg8"},"source":["# Tokenization"]},{"cell_type":"code","metadata":{"id":"yKwbpeN_PMiu","colab":{"base_uri":"https://localhost:8080/","height":281},"executionInfo":{"status":"ok","timestamp":1621159109225,"user_tz":420,"elapsed":69165,"user":{"displayName":"ABHISHEK RANJAN","photoUrl":"","userId":"15498253178039914064"}},"outputId":"d4fef997-2d71-4aa9-c5d1-bfadbff89cf8"},"source":["# get length of all the messages in the train set\n","seq_len = [len(i.split()) for i in train_text]\n","\n","pd.Series(seq_len).hist(bins = 30)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7f7c7bb33610>"]},"metadata":{"tags":[]},"execution_count":13},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"OXcswEIRPvGe"},"source":["max_seq_len = 100"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tk5S7DWaP2t6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621159110176,"user_tz":420,"elapsed":70103,"user":{"displayName":"ABHISHEK RANJAN","photoUrl":"","userId":"15498253178039914064"}},"outputId":"8aabe273-d651-4499-8fac-4311ea668661"},"source":["# tokenize and encode sequences in the training set\n","tokens_train = tokenizer.batch_encode_plus(\n","    train_text.tolist(),\n","    max_length = max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n","\n","# tokenize and encode sequences in the validation set\n","tokens_val = tokenizer.batch_encode_plus(\n","    val_text.tolist(),\n","    max_length = max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n","\n","# tokenize and encode sequences in the test set\n","tokens_test = tokenizer.batch_encode_plus(\n","    test_text.tolist(),\n","    max_length = max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"Wsm8bkRZQTw9"},"source":["# Convert Integer Sequences to Tensors"]},{"cell_type":"code","metadata":{"id":"QR-lXwmzQPd6"},"source":["# for train set\n","train_seq = torch.tensor(tokens_train['input_ids'])\n","train_mask = torch.tensor(tokens_train['attention_mask'])\n","train_y = torch.tensor(train_labels.tolist())\n","\n","# for validation set\n","val_seq = torch.tensor(tokens_val['input_ids'])\n","val_mask = torch.tensor(tokens_val['attention_mask'])\n","val_y = torch.tensor(val_labels.tolist())\n","\n","# for test set\n","test_seq = torch.tensor(tokens_test['input_ids'])\n","test_mask = torch.tensor(tokens_test['attention_mask'])\n","test_y = torch.tensor(test_labels.tolist())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ov1cOBlcRLuk"},"source":["# Create DataLoaders"]},{"cell_type":"code","metadata":{"id":"qUy9JKFYQYLp"},"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","#define a batch size\n","batch_size = 32\n","\n","# wrap tensors\n","train_data = TensorDataset(train_seq, train_mask, train_y)\n","\n","# sampler for sampling the data during training\n","train_sampler = RandomSampler(train_data)\n","\n","# dataLoader for train set\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# wrap tensors\n","val_data = TensorDataset(val_seq, val_mask, val_y)\n","\n","# sampler for sampling the data during training\n","val_sampler = SequentialSampler(val_data)\n","\n","# dataLoader for validation set\n","val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K2HZc5ZYRV28"},"source":["# Freeze BERT Parameters"]},{"cell_type":"code","metadata":{"id":"wHZ0MC00RQA_"},"source":["# freeze all the parameters\n","for param in bert.parameters():\n","    param.requires_grad = True\n","# print(bert)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jreqIBvmN8ez"},"source":["class BiRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes,dropout):\n","        super(BiRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True,dropout=dropout, bidirectional=True)\n","        self.fc = nn.Linear(hidden_size*2, num_classes)  # 2 for bidirection\n","    \n","    def forward(self, x):\n","        # Set initial states\n","        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device) # 2 for bidirection \n","        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device)\n","        \n","        # Forward propagate LSTM\n","        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size*2)\n","        \n","        # Decode the hidden state of the last time step\n","        out = self.fc(out[:, -1, :])\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s7ahGBUWRi3X"},"source":["# Define Model Architecture"]},{"cell_type":"code","metadata":{"id":"b3iEtGyYRd0A"},"source":["class BERT_Arch(nn.Module):\n","\n","    def __init__(self, bert):\n","      \n","      super(BERT_Arch, self).__init__()\n","\n","      self.bert = bert \n","      \n","      # dropout layer\n","      # self.dropout = nn.Dropout(0.1)\n","      \n","      # relu activation function\n","      # self.relu =  nn.ReLU()\n","\n","      # dense layer 1\n","      # self.fc1 = nn.Linear(32768,768)\n","\n","      \n","      # self.fc1_2= nn.GRU(input_size=512, hidden_size=100, num_layers=1, batch_first=False, bidirectional=True)\n","      self.fc1_2 = BiRNN(input_size=768, hidden_size=1024, num_layers=2,num_classes=512,dropout=0.25).to(device)\n","      # self.fc_2 = BiRNN(input_size=768, hidden_size=1024, num_layers=1,num_classes=512,droput=0.25).to(device)\n","      # dense layer 2 (Output layer)\n","      # self.n = NetVLAD(num_clusters=32, dim=512, alpha=1.0)\n","      # self.model = EmbedNet(base_model, net_vlad).cuda()\n","      self.fc2 = nn.Linear(512,2)\n","\n","      #softmax activation function\n","      self.softmax = nn.LogSoftmax(dim=1)\n","\n","    #define the forward pass\n","    def forward(self, sent_id, mask):\n","\n","      #pass the inputs to the model  \n","      _, cls_hs = self.bert(sent_id,mask, return_dict=False)\n","      \n","      # x = self.fc1(cls_hs)\n","\n","      # x = self.relu(x)\n","\n","      # x = self.dropout(x)\n","       \n","      # x=x.reshape((x.size, 1))\n","\n","      x = cls_hs.view(-1,1,768)\n","\n","      x = self.fc1_2(x)\n","      # x = x.view(-1,512,1,1)\n","      # x= self.n(x)\n","      # output layer\n","      x = self.fc2(x)\n","      \n","      # apply softmax activation\n","      x = self.softmax(x)\n","\n","      return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kpSZVA4qn5QK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cBAJJVuJRliv"},"source":["# pass the pre-trained BERT to our define architecture\n","base_model = BERT_Arch(bert)\n","# myObject = myClass()\n","# myObject.forward(a,b)\n","# push the model to GPU\n","base_model = base_model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cw90s_Jfn_Vi"},"source":["from torch.autograd import Variable\n","\n","from netvlad import NetVLAD\n","from netvlad import EmbedNet\n","from hard_triplet_loss import HardTripletLoss\n","\n","\n","# Discard layers at the end of base network\n","\n","dim = list(base_model.parameters())[-1].shape[0]  # last channels (512)\n","\n","# Define model for embedding\n","net_vlad = NetVLAD(num_clusters=32, dim=dim, alpha=1.0)\n","model = EmbedNet(base_model, net_vlad).cuda()\n","\n","# Define loss\n","criterion = HardTripletLoss(margin=0.1).cuda()\n","\n","# This is just toy example. Typically, the number of samples in each classes are 4.\n","# labels = torch.randint(0, 10, (40, )).long()\n","# x = torch.rand(40, 3, 128, 128).cuda()\n","# output = model(x)\n","\n","# triplet_loss = criterion(output, labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nbgr79jlqFb-"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"taXS0IilRn9J"},"source":["# optimizer from hugging face transformers\n","from transformers import AdamW\n","\n","# define the optimizer\n","optimizer = AdamW(model.parameters(), lr = 1e-6)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zDZmPV48n6fq"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j9CDpoMQR_rK"},"source":["# Find Class Weights"]},{"cell_type":"code","metadata":{"id":"izY5xH5eR7Ur","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621159865576,"user_tz":420,"elapsed":1517,"user":{"displayName":"ABHISHEK RANJAN","photoUrl":"","userId":"15498253178039914064"}},"outputId":"d4cd0dd6-82be-4ab9-a615-f47231b4cc26"},"source":["from sklearn.utils.class_weight import compute_class_weight\n","\n","#compute the class weights\n","class_wts = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n","\n","print(class_wts)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[1. 1.]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"r1WvfY2vSGKi"},"source":["# convert class weights to tensor\n","weights= torch.tensor(class_wts,dtype=torch.float)\n","weights = weights.to(device)\n","\n","# loss function\n","# cross_entropy  = nn.NLLLoss(weight=weights) \n","# triplet_loss = criterion(output, labels)\n","\n","# number of training epochs\n","epochs = 10"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"My4CA0qaShLq"},"source":["# Fine-Tune BERT"]},{"cell_type":"code","metadata":{"id":"rskLk8R_SahS"},"source":["# function to train the model\n","def train():\n","  \n","  model.train()\n","\n","  total_loss, total_accuracy = 0, 0\n","  \n","  # empty list to save model predictions\n","  total_preds=[]\n","  \n","  # iterate over batches\n","  for step,batch in enumerate(train_dataloader):\n","    \n","    # progress update after every 50 batches.\n","    if step % 50 == 0 and not step == 0:\n","      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n","\n","    # push the batch to gpu\n","    batch = [r.to(device) for r in batch]\n"," \n","    sent_id, mask, labels = batch\n","\n","    # clear previously calculated gradients \n","    model.zero_grad()        \n","\n","    # get model predictions for the current batch\n","    preds = model(sent_id, mask)\n","\n","    # compute the loss between actual and predicted values\n","    loss = cross_entropy(preds, labels)\n","    # loss = criterion(output, labels)\n","    # add on to the total loss\n","    total_loss = total_loss + loss.item()\n","\n","    # backward pass to calculate the gradients\n","    loss.backward()\n","\n","    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","    # update parameters\n","    optimizer.step()\n","\n","    # model predictions are stored on GPU. So, push it to CPU\n","    preds=preds.detach().cpu().numpy()\n","\n","    # append the model predictions\n","    total_preds.append(preds)\n","\n","  # compute the training loss of the epoch\n","  avg_loss = total_loss / len(train_dataloader)\n","  \n","  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n","  # reshape the predictions in form of (number of samples, no. of classes)\n","  total_preds  = np.concatenate(total_preds, axis=0)\n","\n","  #returns the loss and predictions\n","  return avg_loss, total_preds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yGXovFDlSxB5"},"source":["# function for evaluating the model\n","def evaluate():\n","  \n","  print(\"\\nEvaluating...\")\n","  \n","  # deactivate dropout layers\n","  model.eval()\n","\n","  total_loss, total_accuracy = 0, 0\n","  \n","  # empty list to save the model predictions\n","  total_preds = []\n","\n","  # iterate over batches\n","  for step,batch in enumerate(val_dataloader):\n","    \n","    # Progress update every 50 batches.\n","    if step % 50 == 0 and not step == 0:\n","      \n","      # Calculate elapsed time in minutes.\n","      # elapsed = format_time(time.time() - t0)\n","            \n","      # Report progress.\n","      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n","\n","    # push the batch to gpu\n","    batch = [t.to(device) for t in batch]\n","\n","    sent_id, mask, labels = batch\n","\n","    # deactivate autograd\n","    with torch.no_grad():\n","      \n","      # model predictions\n","      preds = model(sent_id, mask)\n","\n","      # compute the validation loss between actual and predicted values\n","      loss = cross_entropy(preds,labels)\n","      # loss = criterion(output, labels)\n","\n","      total_loss = total_loss + loss.item()\n","\n","      preds = preds.detach().cpu().numpy()\n","\n","      total_preds.append(preds)\n","\n","  # compute the validation loss of the epoch\n","  avg_loss = total_loss / len(val_dataloader) \n","\n","  # reshape the predictions in form of (number of samples, no. of classes)\n","  total_preds  = np.concatenate(total_preds, axis=0)\n","\n","  return avg_loss, total_preds"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9KZEgxRRTLXG"},"source":["# Start Model Training"]},{"cell_type":"code","metadata":{"id":"k1USGTntS3TS","colab":{"base_uri":"https://localhost:8080/","height":408},"executionInfo":{"status":"error","timestamp":1621159972757,"user_tz":420,"elapsed":2253,"user":{"displayName":"ABHISHEK RANJAN","photoUrl":"","userId":"15498253178039914064"}},"outputId":"9cff7b3a-6c13-438c-b554-1039f0b1a287"},"source":["# # set initial loss to infinite\n","# import torch\n","# torch.cuda.empty_cache()\n","# import gc\n","# # del variables\n","# gc.collect()\n","\n","\n","best_valid_loss = float('inf')\n","\n","# empty lists to store training and validation loss of each epoch\n","train_losses=[]\n","valid_losses=[]\n","\n","#for each epoch\n","epochs=2\n","for epoch in range(epochs):\n","     \n","    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n","    \n","    #train model\n","    train_loss,_ = train()\n","    \n","    #evaluate model\n","    valid_loss, _ = evaluate()\n","    \n","    #save the best model\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'saved_weights.pt')\n","    \n","    # append training and validation loss\n","    train_losses.append(train_loss)\n","    valid_losses.append(valid_loss)\n","    \n","    print(f'\\nTraining Loss: {train_loss:.3f}')\n","    print(f'Validation Loss: {valid_loss:.3f}')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n"," Epoch 1 / 2\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-57-943b40e80261>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m#train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m#evaluate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-53-6a2d0ff9432a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# get model predictions for the current batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# compute the loss between actual and predicted values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"]}]},{"cell_type":"markdown","metadata":{"id":"_yrhUc9kTI5a"},"source":["# Load Saved Model"]},{"cell_type":"code","metadata":{"id":"OacxUyizS8d1"},"source":["#load weights of best model\n","path = 'saved_weights.pt'\n","model.load_state_dict(torch.load(path))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x4SVftkkTZXA"},"source":["# Get Predictions for Test Data"]},{"cell_type":"code","metadata":{"id":"NZl0SZmFTRQA"},"source":["# get predictions for test data\n","import torch\n","torch.cuda.empty_cache()\n","import gc\n","# del variables\n","gc.collect()\n","\n","with torch.no_grad():\n","  preds = model(test_seq.to(device), test_mask.to(device))\n","  preds = preds.detach().cpu().numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ms1ObHZxTYSI"},"source":["# model's performance\n","preds = np.argmax(preds, axis = 1)\n","print(classification_report(test_y, preds))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YqzLS7rHTp4T"},"source":["# confusion matrix\n","# pd.crosstab(test_y, preds)\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","\n","print(accuracy_score(test_y, preds))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jpX1uTwjUPY6"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BY2P7H1W2Wo4"},"source":[""],"execution_count":null,"outputs":[]}]}